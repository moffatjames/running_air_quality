---
title: "Best hour to run"
author: "James Johnson"
date: "July 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## I'm a bit of a dork about air pollution

I'm currently about to start a PhD with a focus on whether air pollution can make people sick. As a result, I'm constantly thinking about the impact of the environment on the lives we live.
 I'm a city dweller, meaning when i go for a run, it's goign to be in an urban environment, near a bunch of cars, trucks, lawnmowers, bbqs, smokers and other potential noise and pollution sources. What bothers me about most air quality research is the time resolution of the data. 
 
 In my field, we often make claims about air quality based on the annual concentrations of pollution in an area. This is really important for policy decisions, because we can say with certainty that, for example, Hamilton, is consistently more polluted than Toronto. This is thanks in part to more industrial activity, and also because of the Niagara escarpment trapping pollution inside it like a fish bowl. These annual concentrations don't, however, tell me when the best time would be to go for a run, if i was interested in experiencing a bit less pollution. So being the aforementioned dork that I am, I decided to explore this a bit further with a bit of R code.
 
### The MECP website

There's a lot of content to explore on the Ministry of Environment, Conservation and Parks (MECP) website. Historic reports, and lots of different summaries of the air pollution data captured across Ontario. You can get recent data (from the past 2 years, and even close to live data) from the website as it exists. But for this example, i'm going to use the data that's already gone through a Quality check from the MECP - this (as of July 15, 2019) means 2017 and earlier.

I'll be exploring today using the rvest package to scrape some air data from the MECP, to try to figure out what might be the best time of the day, from an air pollution perspective, to go for a run in Downtown Toronto. At a future date, I'll be posting a tutorial to walk through how I scraped the Ministry website using R.

First, the [website](http://www.airqualityontario.com/history/):

![MECP website](assets/Pollutant_history.PNG)

As you can see, I've chosen some defaults to explore Toronto's Downtown site, and i've not completed the end month, with the hope that i'll just get the whole year of data:

![Success!!](assets/data_capture.PNG)

If I scroll down, it looks like a full year of data! It's a bit of a mess though - the columns of hour data are goign to make it very hard to summarize. I'm going to use the `rvest` and some `tidyverse` packages to try to see if I can get the answer I'm looking for. Thanks to [this helpful tutorial](https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/) for getting me started with `rvest`.  

```{r}

#Loading the rvest package
library(rvest)
library(tidyverse)
library(lubridate)
source(file = '1_ref_tables.R')
#Specifying the url for desired website to be scraped
url <- "http://www.airqualityontario.com/history/?c=Personal&s=31103&y=2017&p=36&m=1&e=&t=html&submitter=Search&i=1"

webpage <- read_html(url)

poln_table <- html_nodes(webpage,'table')

rank_data <- html_table(poln_table)[[1]]

rank_data %>% 
  gather(Hour, Conc, H01:H24) %>%
  mutate(Conc =ifelse(Conc == 9999|Conc == -999, NA, Conc),
         month = month(Date),
         season = ifelse(month >=10|month <=3,"Winter","Construction")) %>% 
  arrange(Date) %>%
  ggplot(aes(Hour, Conc)) +
  geom_boxplot() +
  facet_wrap(~season)
  
start_year <- 2013
end_year <- 2017
all_data <- tibble()
for(year in start_year:end_year){
  url <- paste0("http://www.airqualityontario.com/history/?c=Personal&s=31103&y=",year,"&p=36&m=1&e=&t=html&submitter=Search&i=1")
  webpage <- read_html(url)
  poln_table <- html_nodes(webpage,'table')
  rank_data <- html_table(poln_table)[[1]]
  all_data <- rbind(all_data,rank_data)
}

all_data %>% 
  gather(Hour, Conc, H01:H24) %>%
  mutate(Conc =ifelse(Conc == 9999|Conc == -999, NA, Conc),
         month = month(Date),
         season = ifelse(month >=10|month <=3,"Winter","Construction")) %>% 
  arrange(Date) %>%
  ggplot(aes(Hour, Conc)) +
  geom_boxplot() +
  facet_wrap(~season)

AQHI <- pollution_codes[str_which(pollution_codes$pollutant,"PM2.5|NO2|O3"),] %>% 
  mutate(name = str_extract(pollutant, "PM2.5|NO2|O3"))

start_year <- 2013
end_year <- 2017
all_data <- tibble()
for(year in start_year:end_year){
  station_URL <-  paste0("http://www.airqualityontario.com/history/?c=Personal&s=31103&y=",year,"&",AQHI$poll_code[1],"&m=1&e=&t=html&submitter=Search&i=1")
  webpage <- read_html(station_URL)
  poln_table <- html_nodes(webpage,'table')
  station_data <- html_table(poln_table)[[1]] %>% 
    gather(Hour, Conc, H01:H24) %>%
  arrange(Date) %>% 
  select(-Conc)
  
  for(pollutant in 1:3){
    url <- paste0("http://www.airqualityontario.com/history/?c=Personal&s=31103&y=",year,"&",AQHI$poll_code[pollutant],"&m=1&e=&t=html&submitter=Search&i=1")
    webpage <- read_html(url)
    poln_table <- html_nodes(webpage,'table')
    poll_data <- html_table(poln_table)[[1]] %>% 
      gather(Hour, Conc, H01:H24) %>%
      arrange(Date)%>% 
      select(Date, Hour, Conc)
    names(poll_data)[3] <-  AQHI$name[pollutant]
  
    station_data <- left_join(station_data,poll_data)
  }
  all_data <- rbind(all_data, station_data)
}


saveRDS(all_data,"DT-toronto2013-17")

DT_tor <- all_data %>% 
  mutate(month = month(Date),
         season = ifelse(month >=10|month <=3,"Winter","Construction")) %>%
  gather(key = poll_type, Conc, NO2:PM2.5) %>% 
  mutate(Conc =ifelse(Conc == 9999|Conc == -999, NA, Conc))

ggplot(DT_tor, aes(Hour,Conc, fill = poll_type,col = poll_type)) +
  geom_boxplot() +
  facet_wrap(~season)


```






